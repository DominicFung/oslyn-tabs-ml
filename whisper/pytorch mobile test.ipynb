{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Whisper\n",
    "\n",
    "OpenAI Whisper is one of the many state-of-the-art, ASR models. \n",
    "\n",
    "- Run Pretrained Model\n",
    "- Transform Pretrained Model to TFlite OR TensorFlow Mobile\n",
    "- Validate Integration with NextJS\n",
    "- Validate Integration with React Native\n",
    "- Provide Additional Training Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall -q -y optimum-intel optimum\n",
    "!pip install -q transformers onnx \"git+https://github.com/eaidova/optimum-intel.git@ea/whisper\" --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "!pip install -q \"openvino>=2023.2.0\" datasets  \"gradio>=4.0\" \"librosa\" \"soundfile\"\n",
    "!pip install -q \"nncf>=2.6.0\" \"jiwer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq, AutoTokenizer\n",
    "\n",
    "model_id = \"openai/whisper-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "pt_model = AutoModelForSpeechSeq2Seq.from_pretrained(model_id)\n",
    "pt_model.eval();\n",
    "\n",
    "tokenizer.save_pretrained(save_directory=\"./model\")\n",
    "pt_model.save_pretrained(save_directory=\"./model\")\n",
    "\n",
    "print(\"models saved to file system.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pip install -q onnx onnxruntime 'optimum[exporters]'\n",
    "!optimum-cli export onnx --model model onnx/ --task 'automatic-speech-recognition'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
